"""Core Agent functionality for building LLM-powered agents with [cosette](https://github.com/AnswerDotAI/cosette)."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../00_core.ipynb.

# %% auto 0
__all__ = ['solve_math', 'Agent']

# %% ../00_core.ipynb 4
from fastcore.utils import *
from fastcore.basics import patch
from dataclasses import dataclass, field
from typing import Any, Optional, List, Callable, Dict
from cosette import Chat, contents, wrap_latex, models
from IPython.display import display, Markdown



# %% ../00_core.ipynb 7
import re, math
import math

def solve_math(
    expression: str  # Mathematical expression as a string (e.g. "2+2", "sqrt(16)")
) -> float:         # Numerical result of the evaluation
    """Evaluates mathematical expressions using a safe subset of Python's math operations.
    
    The tool supports these operations:
    - Basic arithmetic: +, -, *, /
    - Functions: sqrt, pow, sin, cos
    - Constants: pi
    
    Examples:
        >>> solve_math("2+2")
        4.0
        >>> solve_math("sqrt(16)")
        4.0
        >>> solve_math("sin(pi/2)")
        1.0
        
    Input Format:
        - Use standard mathematical notation
        - Write functions in lowercase: sqrt(), sin(), cos()
        - Use parentheses for function arguments: sqrt(16)
        
    Safety:
        - Only whitelisted math operations are allowed
        - No arbitrary Python code execution
    """
    namespace = {
        'sqrt': math.sqrt,
        'pow': math.pow,
        'sin': math.sin,
        'cos': math.cos,
        'pi': math.pi
    }
    return eval(expression, {"__builtins__": {}}, namespace)

# %% ../00_core.ipynb 10
@dataclass
class Agent:
    """An Agent that can perform tasks using an LLM and optional tools.
    
    The Agent maintains its own conversation state and can use tools to perform
    actions. It follows Anthropic's best practices for tool usage and prompting.
    
    Args:
        role: Description of agent's role (e.g. "math tutor")
        model: LLM model to use (from cosette.models)
        tools: Optional list of callable tools with type hints and docstrings
        system: Override default system prompt
        memory_size: Number of conversation turns to retain
    
    Example:
        ```python
        # Create a math tutor agent
        math_agent = Agent(
            role="math tutor",
            model="gpt-4o",
            tools=[solve_math],
            system="You are a helpful math tutor. Show your work and verify with tools."
        )
        
        # Use the agent
        response = math_agent.run_with_tools("What is sqrt(16) + 7?")
        ```
    """
    role: str
    model: str
    tools: List[Callable] = field(default_factory=list)
    system: Optional[str] = None
    memory_size: int = 10
    
    def __post_init__(self):
        """Initialize the agent with model and system prompt."""
        if self.model not in models: 
            raise ValueError(f"Model {self.model} not in available models: {models}")
        self.chat = Chat(self.model, tools=self.tools)
        self.chat.sp = self.system or f"You are a {self.role}."


# %% ../00_core.ipynb 12
@patch
def run_with_tools(self:Agent, prompt:str, max_steps:int=5, **kwargs) -> str:
    """Execute a conversation turn with automatic tool usage.
    
    Uses cosette's toolloop to allow the model to:
    1. Analyze the prompt
    2. Choose appropriate tools
    3. Call tools with proper parameters
    4. Use results to form response
    
    Args:
        prompt: User's input message
        max_steps: Maximum number of tool calls (default: 5)
        **kwargs: Additional arguments passed to toolloop
    
    Returns:
        The model's final response after tool usage
    
    Example:
        ```python
        agent = Agent(role="math tutor", model="gpt-4o", tools=[solve_math])
        response = agent.run_with_tools("What is sqrt(16) + sin(pi/2)?")
        ```
    """
    self._prune_history()
    response = self.chat.toolloop(prompt, max_steps=max_steps, **kwargs)
    return contents(response)

# %% ../00_core.ipynb 13
@patch
def show(self:Agent):
    """Display agent configuration and conversation history.
    
    Shows:
    - Current role and model
    - System prompt
    - Available tools
    - Token usage statistics
    - Full conversation history
    """
    config_md = f"""
# Agent Configuration

**Role**: {self.role}  
**Model**: {self.model}  
**System**: {self.chat.sp}  
**Memory Size**: {self.memory_size}  
**Tools**: {len(self.tools)} - {', '.join(t.__name__ for t in self.tools)}

## Token Usage
{self.chat.use}

## Conversation History
{self._format_history()}
"""
    display(Markdown(config_md))



# %% ../00_core.ipynb 14
@patch
def _format_history(self:Agent):
    """Format conversation history for markdown display."""
    lines = []
    if hasattr(self.chat, 'h') and self.chat.h:
        for msg in self.chat.h:
            role = msg.role.capitalize()
            content = msg.content or ""
            lines.append(f"**{role}:** {content}")
    return "\n\n".join(lines)

# %% ../00_core.ipynb 15
@patch
def _prune_history(self:Agent):
    """Maintain conversation history within memory_size limit."""
    if self.memory_size is None or self.memory_size <= 0: return
    if hasattr(self.chat, 'h') and len(self.chat.h) > (self.memory_size + 1):
        system_msgs = [msg for msg in self.chat.h if msg.role == 'system']
        other_msgs = [msg for msg in self.chat.h if msg.role != 'system'][-self.memory_size:]
        self.chat.h = system_msgs + other_msgs
